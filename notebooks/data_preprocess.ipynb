{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, lit, date_add, explode, sequence\n",
    "from pyspark.sql.functions import corr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/03 20:00:04 WARN Utils: Your hostname, Khim3 resolves to a loopback address: 127.0.1.1; using 192.168.75.108 instead (on interface wlo1)\n",
      "25/01/03 20:00:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/03 20:00:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(master = 'local')\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"Python Spark Data Preprocessing\") \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|2018-02-05|     262.0|267.899994|250.029999|254.259995|254.259995|11896100|\n",
      "|2018-02-06|247.699997|266.700012|     245.0|265.720001|265.720001|12595800|\n",
      "|2018-02-07|266.579987|272.450012|264.329987|264.559998|264.559998| 8981500|\n",
      "|2018-02-08|267.079987|267.619995|     250.0|250.100006|250.100006| 9306700|\n",
      "|2018-02-09|253.850006|255.800003|236.110001|249.470001|249.470001|16906900|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = spark.read.csv('../data/NFLX.csv', header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check null values, schema, and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+---------+------+\n",
      "|Date|Open|High|Low|Close|Adj Close|Volume|\n",
      "+----+----+----+---+-----+---------+------+\n",
      "|   0|   0|   0|  0|    0|        0|     0|\n",
      "+----+----+----+---+-----+---------+------+\n",
      "\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n",
      "Number of rows: 1009\n",
      "Number of columns: 7\n"
     ]
    }
   ],
   "source": [
    "# 1. Check for null values\n",
    "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()\n",
    "\n",
    "# 2. Get schema and row/column counts\n",
    "df.printSchema()\n",
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "\n",
    "# 3. Drop duplicates\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|2018-08-08|352.209991|352.290009|346.609985|347.609985|347.609985| 5394700|\n",
      "|2018-09-11|344.670013|356.149994|343.899994|355.929993|355.929993| 6198100|\n",
      "|2019-07-09|379.059998| 384.76001|     377.5|379.929993|379.929993| 6932800|\n",
      "|2020-01-15|338.679993|343.170013|336.600006|339.070007|339.070007| 5158000|\n",
      "|2020-08-10|493.350006|497.459991|478.630005|483.380005|483.380005| 4691200|\n",
      "|2021-06-08|     497.0|498.820007|489.369995|492.390015|492.390015| 2374000|\n",
      "|2019-01-07|302.100006|316.799988|301.649994|315.339996|315.339996|18620100|\n",
      "|2019-06-10|363.649994|367.100006|349.290009| 352.01001| 352.01001| 7810300|\n",
      "|2020-07-28|496.019989|497.790009| 487.76001| 488.51001| 488.51001| 5986700|\n",
      "|2020-08-31|521.159973| 531.98999|517.580017|529.559998|529.559998| 4941400|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/03 20:00:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|              Open|              High|               Low|             Close|         Adj Close|           Volume|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|              1009|              1009|              1009|              1009|              1009|             1009|\n",
      "|   mean|419.05967286224006|425.32070308027704|412.37404380178464|419.00073292071335|419.00073292071335| 7570685.03468781|\n",
      "| stddev|108.53753170401455|109.26295957119457|107.55586739006048|108.28999877034995|108.28999877034995|5465535.225689977|\n",
      "|    min|        233.919998|        250.649994|        231.229996|        233.880005|        233.880005|          1144000|\n",
      "|    max|        692.349976|         700.98999|        686.090027|        691.690002|        691.690002|         58904300|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Close and Open: 0.9968121105128466\n",
      "Correlation between Close and High: 0.9985508166555647\n",
      "Correlation between Close and Low: 0.9985438753668088\n",
      "Correlation between Close and Volume: -0.4133618746720189\n"
     ]
    }
   ],
   "source": [
    "# List of columns to calculate correlation with 'Close'\n",
    "columns_to_corr = [col for col in df.columns if col not in [\"Close\", \"Adj Close\", \"Date\"]]\n",
    "\n",
    "# Calculate correlations and show results\n",
    "correlations = [\n",
    "    df.select(corr(\"Close\", col).alias(f\"Close_{col}_Corr\")).collect()[0][0] \n",
    "    for col in columns_to_corr\n",
    "]\n",
    "\n",
    "# Display correlations\n",
    "for col, corr_value in zip(columns_to_corr, correlations):\n",
    "    print(f\"Correlation between Close and {col}: {corr_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The dataset is pretty clean and could be used for modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set row count: 807\n",
      "Testing set row count: 202\n",
      "+----------+----------+----------+----------+----------+----------+-------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close| Volume|\n",
      "+----------+----------+----------+----------+----------+----------+-------+\n",
      "|2021-06-08|     497.0|498.820007|489.369995|492.390015|492.390015|2374000|\n",
      "|2021-10-05|606.940002|640.390015|606.890015|634.809998|634.809998|9534300|\n",
      "|2021-10-22|651.809998|665.460022|651.809998|664.780029|664.780029|6186000|\n",
      "|2021-11-08|650.289978|     656.0|643.789978|651.450012|651.450012|2887500|\n",
      "|2021-11-17|     690.0| 700.98999|686.090027|691.690002|691.690002|2732800|\n",
      "+----------+----------+----------+----------+----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# order the dates in ascending order\n",
    "df = df.orderBy('Date')\n",
    "# Calculate split index\n",
    "split_index = int(df.count() * 0.8)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train = df.limit(split_index)  # Take the first 80% of rows\n",
    "test = df.subtract(train)      # Subtract the training set from the original DataFrame to get the test set\n",
    "test_copy = test.select(\"*\") \n",
    "# Display row counts of the resulting DataFrames to verify the split\n",
    "print(f\"Training set row count: {train.count()}\")\n",
    "print(f\"Testing set row count: {test.count()}\")\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/03 20:00:18 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "test_pandas = test.toPandas()\n",
    "\n",
    "# Convert the Date column to datetime format for accurate sorting\n",
    "test_pandas[\"Date\"] = pd.to_datetime(test_pandas[\"Date\"])\n",
    "\n",
    "# Sort the Pandas DataFrame by the Date column\n",
    "test_pandas = test_pandas.sort_values(by=\"Date\")\n",
    "test_pandas.head()\n",
    "test_pandas.to_csv(\"test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
