{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import altair as alt\n",
    "from pyspark.ml.regression import LinearRegressionModel, DecisionTreeRegressionModel\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, lit, date_add, explode, sequence\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 17:20:44 WARN Utils: Your hostname, Khim3 resolves to a loopback address: 127.0.1.1; using 10.0.111.51 instead (on interface wlo1)\n",
      "24/12/18 17:20:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/18 17:20:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/18 17:20:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Initialize SparkSession using the existing or new SparkContext\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Demo App\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'models/linear_regressor'\n",
    "model = LinearRegressionModel.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|2018-02-07|266.579987|272.450012|264.329987|264.559998|264.559998| 8981500|\n",
      "|2018-02-08|267.079987|267.619995|     250.0|250.100006|250.100006| 9306700|\n",
      "|2018-02-20| 277.73999|285.809998|276.609985|278.549988|278.549988| 7769000|\n",
      "|2018-02-26|    288.75|295.649994| 287.01001|294.160004|294.160004|10268600|\n",
      "|2018-03-05|302.850006|316.910004|297.600006|     315.0|     315.0|18986100|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_test = spark.read.csv('./test.csv', header=True, inferSchema=True)\n",
    "\n",
    "data_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 17:20:56 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# check data type of data_tetst\n",
    "data_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|     label|\n",
      "+--------------------+----------+\n",
      "|[266.579987,272.4...|264.559998|\n",
      "|[267.079987,267.6...|250.100006|\n",
      "|[277.73999,285.80...|278.549988|\n",
      "|[288.75,295.64999...|294.160004|\n",
      "|[302.850006,316.9...|     315.0|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if \"features\" in data_test.columns:\n",
    "    data_test = data_test.drop(\"features\")\n",
    "feature_columns = [\"Open\", \"High\", \"Low\", \"Volume\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df = assembler.transform(data_test).select(\"features\", col(\"Close\").alias(\"label\"))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|            features| label|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|[255.710007,266.6...|264.75|262.95816627665533|\n",
      "+--------------------+------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 17:21:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "df = df.orderBy(col(\"features\")).limit(1)\n",
    "predictions = model.transform(df)        \n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
