{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb438e6-5cc5-40b3-aaa0-86468763e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "import pyspark.sql.functions as F\n",
    "from wrapper import RegressorWrapper, LSTMWrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8fafb-b051-44a7-bf6d-57a58e6fa0cf",
   "metadata": {},
   "source": [
    "# Chapter 1: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5032e8-bca8-43d8-b16e-c8a804f7ded8",
   "metadata": {},
   "source": [
    "## 1.1. Load data to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d87a7f0-33bf-420b-bcab-8c93bd79c278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/12 21:35:46 WARN Utils: Your hostname, WindowEnv resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/01/12 21:35:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/12 21:35:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Netflix Stock Prediction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe9ea779650>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Netflix Stock Prediction\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c046df2-3ced-44d7-810e-2c400ed33142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_rows: 1009; n_cols7\n",
      "\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|2018-02-05|     262.0|267.899994|250.029999|254.259995|254.259995|11896100|\n",
      "|2018-02-06|247.699997|266.700012|     245.0|265.720001|265.720001|12595800|\n",
      "|2018-02-07|266.579987|272.450012|264.329987|264.559998|264.559998| 8981500|\n",
      "|2018-02-08|267.079987|267.619995|     250.0|250.100006|250.100006| 9306700|\n",
      "|2018-02-09|253.850006|255.800003|236.110001|249.470001|249.470001|16906900|\n",
      "|2018-02-12|252.139999|259.149994|     249.0|257.950012|257.950012| 8534900|\n",
      "|2018-02-13|257.290009|261.410004|254.699997|258.269989|258.269989| 6855200|\n",
      "|2018-02-14|260.470001|269.880005|260.329987|     266.0|     266.0|10972000|\n",
      "|2018-02-15|270.029999|     280.5|267.630005|280.269989|280.269989|10759700|\n",
      "|2018-02-16|278.730011|281.959991|275.690002|278.519989|278.519989| 8312400|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./data/NFLX.csv', header=True, inferSchema=True)\n",
    "print(f'n_rows: {df.count()}; n_cols{len(df.columns)}\\n')\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ec7f4-1a7a-4d98-95b2-0c52f14f82a6",
   "metadata": {},
   "source": [
    "## 1.2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6835ab15-4e2b-48db-a8d8-7120792f0b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+---------+------+\n",
      "|Date|Open|High|Low|Close|Adj Close|Volume|\n",
      "+----+----+----+---+-----+---------+------+\n",
      "|   0|   0|   0|  0|    0|        0|     0|\n",
      "+----+----+----+---+-----+---------+------+\n",
      "\n",
      "1009 \n",
      "\n",
      "['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume']\n"
     ]
    }
   ],
   "source": [
    "# null-values (no null)\n",
    "df.select([F.sum(F.col(c).isNull().cast('int')).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# duplicates (no dup)\n",
    "print(df.drop_duplicates().count(), \"\\n\")\n",
    "\n",
    "# naming convenient\n",
    "df = df.toDF(*[c.lower() for c in df.columns])\n",
    "df = df.withColumnRenamed('adj close', 'adj_close')\n",
    "print(df.columns)\n",
    "\n",
    "# change date to ascending order\n",
    "df = df.orderBy('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69036c-171c-436a-bb58-10fd7c8a19bf",
   "metadata": {},
   "source": [
    "## 1.3. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088109c4-ad19-4401-81cd-9c6e112c385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open: 0.9968121105128477\n",
      "high: 0.9985508166555668\n",
      "low: 0.9985438753668097\n",
      "volume: -0.41336187467202007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/12 21:35:58 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# identify target and features\n",
    "target = 'close'\n",
    "features = [c for c in df.columns if c not in ['close', 'adj_close', 'date']]\n",
    "\n",
    "# calculate correlations\n",
    "for c in features:\n",
    "    corr = df.select(F.corr('close', c)).collect()[0][0]\n",
    "    print(f\"{c}: {corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2166afba-97b7-4566-a8dd-0cd38d2cee60",
   "metadata": {},
   "source": [
    "# Chapter 2: Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e289f3-fcef-4074-895a-2bfec76e2696",
   "metadata": {},
   "source": [
    "## 2.1 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6051e396-2be8-4120-b6bb-8290161cbc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'rmse': 3.757339216949421, 'mse': 14.11759799122609, 'r2': 0.99881493996627}\n",
      "test: {'rmse': 4.493396485242403, 'mse': 20.190611973588783, 'r2': 0.99811429537498}\n",
      "model: PipelineModel_2b51f526aeba\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "ev = RegressionEvaluator()\n",
    "lr_wrapper = RegressorWrapper(lr, ev, target, features, None)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "result = lr_wrapper.create(df, paramGrid, ['rmse', 'mse', 'r2'])\n",
    "\n",
    "for key ,val in result.items():\n",
    "    print(f'{key}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c1fc1-a983-4dba-a9a1-1a3a6ec8fda9",
   "metadata": {},
   "source": [
    "## 2.2 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caff089a-246a-416b-9f73-55eaa7108fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'rmse': 3.0031784769986167, 'mse': 9.019080964707731, 'r2': 0.9992429199075584}\n",
      "test: {'rmse': 6.092744705207209, 'mse': 37.12153804283048, 'r2': 0.9965330295056539}\n",
      "model: PipelineModel_afbbc7e1e52d\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "dt_wrapper = RegressorWrapper(dt, ev, target, features, None)\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64, 128]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 2, 3]) \\\n",
    "    .addGrid(dt.minInfoGain, [0.0, 0.01, 0.02]) \\\n",
    "    .build()\n",
    "\n",
    "result = dt_wrapper.create(df, paramGrid, ['rmse', 'mse', 'r2'])\n",
    "\n",
    "for key ,val in result.items():\n",
    "    print(f'{key}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c0446-981b-4043-bd9b-279dce76f390",
   "metadata": {},
   "source": [
    "## 2.3. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a7b3756-3127-4cea-bc2e-983da7d0d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Train Loss: 0.0004, Val Loss: 0.0004\n",
      "Epoch 20/200, Train Loss: 0.0001, Val Loss: 0.0002\n",
      "Epoch 30/200, Train Loss: 0.0001, Val Loss: 0.0002\n",
      "Epoch 40/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 50/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 60/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 70/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 80/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 90/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 100/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 110/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 120/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 130/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 140/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 150/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 160/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 170/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 180/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 190/200, Train Loss: 0.0000, Val Loss: 0.0001\n",
      "Epoch 200/200, Train Loss: 0.0000, Val Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "lstm_df = df.select([c for c in df.columns if c != 'date'])\n",
    "lstm = LSTMWrapper()\n",
    "lstm.create(lstm_df, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4b286-6c4a-4974-8a99-3edad12c4118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDCKernel",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
